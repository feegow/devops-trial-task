# Alerta baseado em Logs (Loki) - Guia PrÃ¡tico

> **O que Ã© isso?** Um alerta que monitora os logs das suas aplicaÃ§Ãµes e dispara quando tem muita coisa dando errado. Simples assim!

## ğŸ¤” Por que precisamos disso?

VocÃª jÃ¡ deve ter passado por isso: a aplicaÃ§Ã£o comeÃ§a a dar erro, mas vocÃª sÃ³ descobre quando alguÃ©m reclama. Com este alerta, o Loki fica de olho nos logs e te avisa **antes** que vire um problemÃ£o.

**Exemplo real:**
- Sua API comeÃ§ou a retornar erro 500
- Em 2 minutos, o Loki detecta a explosÃ£o de erros
- VocÃª recebe o alerta e jÃ¡ pode agir
- Cliente nem percebe o problema (ou percebe bem menos!)

## ğŸ¯ O Alerta: LogsErrorBurst

### O que ele faz?

Simples: conta quantos logs de erro aparecem por segundo. Se passar de **5 erros/segundo**, ele dispara um alerta.

**Por que 5?** Ã‰ tipo o limite entre "tÃ¡ tudo bem" e "opa, tem algo errado aqui". 

**Fazendo as contas:**
- **SLO**: 99.5% de disponibilidade (error budget de 0.5%)
- **Taxa normal**: ~50 req/s Ã— 0.01% erro = **~0.005 erros/s** (super saudÃ¡vel!)
- **Taxa no limite**: ~50 req/s Ã— 0.5% erro = **~0.25 erros/s** (no limite do SLO)
- **Threshold do alerta**: **5 erros/s** = 1000Ã— acima do normal, ou ~100Ã— acima do limite do SLO
- **ConclusÃ£o**: Se chegou em 5 erros/s, algo estÃ¡ **MUITO** errado!

### Como funciona?

```
AplicaÃ§Ã£o Python/Go 
    â†“
Emite logs JSON: {"level":"error", "route":"/api/...", ...}
    â†“
Promtail coleta os logs
    â†“
Loki armazena e processa
    â†“
Loki Ruler executa a query a cada 1 minuto
    â†“
Se taxa > 5 erros/s por 2 minutos consecutivos
    â†“
ğŸš¨ Alerta dispara no AlertManager!
```

## ğŸ“ A Consulta LogQL

### Query do Alerta (no Loki Ruler)

Esta Ã© a query que vai no arquivo `loki-ruler-config.yaml`:

```logql
sum(rate({namespace="apps"} |~ "error" [5m])) > 5
```

### Query para Visualizar no Grafana

```logql
sum(rate({namespace="apps"} |~ "error" [5m]))
```

---
### Traduzindo para o portuguÃªs:

1. **`{namespace="apps"}`** 
   â†’ Olha sÃ³ os logs das aplicaÃ§Ãµes (nÃ£o os do sistema)

2. **`|~ "error"`**
   â†’ Busca por logs que contÃªm "error"
   â†’ O `|~` Ã© tipo um "contains" com regex

3. **`[5m]`**
   â†’ Nos Ãºltimos 5 minutos

4. **`rate(...)`**
   â†’ Calcula: quantos logs de erro por segundo

5. **`sum(...)`**
   â†’ Soma tudo (de todas as aplicaÃ§Ãµes)

6. **`> 5`**
   â†’ Se passar de 5 erros/segundo â†’ ğŸš¨ ALERTA!

### Exemplos de logs que sÃ£o capturados:

```json
// âœ… Estes disparam o alerta:
{"level":"error", "status":500, ...}           // Tem "error" e "500"
{"level":"ERROR", "route":"/api", ...}         // Tem "ERROR"
{"status":503, "message":"timeout"}            // Status 5xx
{"info":"Database error occurred"}             // Tem "error"

// âŒ Estes NÃƒO disparam:
{"level":"info", "status":200, ...}            // Tudo OK
{"level":"warn", "status":404, ...}            // Warning, mas nÃ£o error
```

## ğŸ·ï¸ Os RÃ³tulos (Labels)

Quando vocÃª olha o alerta ou investiga no Grafana, vocÃª tem acesso a vÃ¡rios **rÃ³tulos** que te ajudam a entender o que tÃ¡ acontecendo:

### Labels AutomÃ¡ticos (Kubernetes)

| Label | O que Ã© | Exemplo | Pra que serve |
|-------|---------|---------|---------------|
| **namespace** | Onde o pod estÃ¡ rodando | `apps` | Separar logs de apps vs sistema |
| **container** | Nome do container | `available-schedules-python` | Filtrar por aplicaÃ§Ã£o |
| **app** | Label do Kubernetes | `available-schedules-python` | Filtrar por aplicaÃ§Ã£o |
| **pod** | Nome do pod especÃ­fico | `available-schedules-python-abc123` | Investigar um pod especÃ­fico |

### Labels ExtraÃ­dos dos Logs JSON

Estes vÃªm do conteÃºdo do log em si (apÃ³s fazer `| json`):

| Label | O que Ã© | Exemplo | Pra que serve |
|-------|---------|---------|---------------|
| **app** | Nome da aplicaÃ§Ã£o (do log) | `available-schedules` | Filtrar por app |
| **route** | Qual endpoint deu erro | `/v1/appoints/available-schedule` | Ver qual rota tÃ¡ falhando |
| **env** | Ambiente | `production`, `staging` | Separar prod de staging |
| **version** | VersÃ£o do cÃ³digo | `v1.0.0`, `v2.0.0` | Ver se erro veio apÃ³s deploy |
| **level** | Severidade | `error`, `info`, `warning` | Filtrar sÃ³ erros |
| **status** | Status HTTP | `200`, `500`, `503` | Ver qual tipo de erro |

## ğŸ” Como Usar na PrÃ¡tica

### 1. Ver logs de erro no Grafana

**Acesse:** http://dev.local/grafana/explore

**Datasource:** Selecione **Loki** no dropdown

**Query bÃ¡sica:**
```logql
{namespace="apps"} |~ "error"
```

**O que vocÃª vÃª:**
- Lista de todos os logs que contÃ©m "error"
- VocÃª pode clicar em cada um para ver detalhes
- DÃ¡ pra ver o timestamp, mensagem completa, etc

### 2. Filtrar por aplicaÃ§Ã£o

**Python (v1):**
```logql
{namespace="apps", container="available-schedules-python"} |~ "error"
```

**Go (v2):**
```logql
{namespace="apps", container="available-schedules-go"} |~ "error"
```

**Ou usando o label `app`:**
```logql
{namespace="apps", app="available-schedules-python"} |~ "error"
```

### 3. Ver a taxa de erro (query do alerta, mas sem a condiÃ§Ã£o!)

**Query simplificada:**
```logql
sum(rate({namespace="apps"} |~ "error" [5m]))
```

**O que aparece:**
- Um nÃºmero tipo `8.5` = 8.5 erros por segundo
- Um grÃ¡fico mostrando como a taxa varia ao longo do tempo
- Se tÃ¡ > 5, vocÃª jÃ¡ sabe: o alerta vai disparar!

**Por aplicaÃ§Ã£o (com `by (container)`):**
```logql
sum by (container) (rate({namespace="apps"} |~ "error" [5m]))
```

## ğŸš¨ Quando o Alerta Dispara

### O que vocÃª recebe

```
ğŸ”´ LogsErrorBurst - FIRING

Taxa de logs de erro: 8.52 logs/segundo
Namespace: apps
Job: available-schedules-python

Janela: 5 minutos
Threshold: >5 logs de erro/segundo

[Ver no Grafana] [Ver Runbook]
```

### O que fazer?

1. **Calma, respira** ğŸ§˜â€â™‚ï¸
   - VocÃª tem tempo, o alerta jÃ¡ te avisou cedo

2. **Abra o Grafana** 
   - Clique no link "Ver no Grafana" do alerta
   - Ou acesse: http://dev.local/grafana/explore

3. **Veja os logs de erro**
   ```logql
   {namespace="apps", job="<o-job-que-alertou>"} | json | level="error"
   ```

4. **Identifique o padrÃ£o**
   - Ã‰ uma rota especÃ­fica?
   - Ã‰ um tipo de erro especÃ­fico?
   - ComeÃ§ou depois de um deploy?

5. **Correlacione com mÃ©tricas**
   - Veja se a latÃªncia tambÃ©m subiu
   - Veja se tem trace no Tempo mostrando o problema
   - Veja as mÃ©tricas HTTP do Prometheus

6. **Aja!**
   - Rollback se foi apÃ³s deploy
   - Escale se Ã© falta de recursos
   - Fix o bug se Ã© cÃ³digo
   - Reinicie o serviÃ§o se tÃ¡ travado

## ğŸ“Š ConfiguraÃ§Ã£o do Alerta

### ParÃ¢metros (vocÃª pode ajustar!)

| ParÃ¢metro | Valor Atual | O que faz | Quando ajustar |
|-----------|-------------|-----------|----------------|
| **Threshold** | `> 5` | Quantos erros/s ativa o alerta | Se tÃ¡ alertando demais, aumenta. Se tÃ¡ quieto demais, diminui |
| **Janela** | `[5m]` | PerÃ­odo que olha no passado | 5min Ã© bom equilÃ­brio |
| **For** | `2m` | Quanto tempo acima do threshold para disparar | Evita alertas de spikes momentÃ¢neos |
| **Severity** | `warning` | Gravidade do alerta | `warning` = investiga, `critical` = acorda Ã s 3h |

## ğŸ“ Arquivos Importantes

### Onde estÃ¡ cada coisa:

```
ğŸ“¦ infra/observability/
â”œâ”€â”€ ğŸ“„ loki-ruler-config.yaml           â† O alerta tÃ¡ aqui!
â”‚   â””â”€â”€ Rules do Loki com query LogQL
â”‚
â”œâ”€â”€ ğŸ“„ values/loki-values.yaml          â† Config do Loki
â”‚   â”œâ”€â”€ Loki Ruler habilitado
â”‚   â””â”€â”€ Pipeline Promtail (extrai labels)
â”‚
â””â”€â”€ ğŸ“„ prometheus-rules.yaml            â† Alertas de mÃ©tricas
    â””â”€â”€ (este Ã© diferente, usa PromQL)

ğŸ“¦ apps/
â”œâ”€â”€ ğŸ“ available-schedules-python/
â”‚   â””â”€â”€ ğŸ“„ main.py                      â† Logs JSON estruturados
â”‚
â””â”€â”€ ğŸ“ available-schedules-go/
    â””â”€â”€ ğŸ“„ main.go                      â† Logs JSON estruturados
```

### Para editar o alerta:

```bash
# 1. Edite a regra
vim infra/observability/loki-ruler-config.yaml

# 2. Aplique
kubectl apply -f infra/observability/loki-ruler-config.yaml

# 3. Reinicie o Loki (para carregar nova regra)
kubectl rollout restart statefulset/loki -n observability

# 4. Aguarde ~1 minuto e a nova regra jÃ¡ estÃ¡ ativa!
```

---

**DÃºvidas?** Abra um issue ou fala com o time de SRE! ğŸš€
