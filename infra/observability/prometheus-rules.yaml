apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: noisy-rules
  namespace: observability
  labels: { release: kps }
spec:
  groups:
    - name: app.recording_rules
      interval: 30s
      rules:
        # Taxa de requisições totais (5min)
        - record: job:http_requests:rate5m
          expr: sum(rate(http_requests_total{job=~"available-schedules-(python|go)"}[5m])) by (job)
          labels:
            metric_type: request_rate
        
        # Taxa de requisições totais (1h)
        - record: job:http_requests:rate1h
          expr: sum(rate(http_requests_total{job=~"available-schedules-(python|go)"}[1h])) by (job)
          labels:
            metric_type: request_rate
        
        # Taxa de erros 5xx (5min)
        - record: job:http_requests_errors_5xx:rate5m
          expr: sum(rate(http_requests_total{job=~"available-schedules-(python|go)",status=~"5.."}[5m])) by (job)
          labels:
            metric_type: error_rate
        
        # Taxa de erros 5xx (1h)
        - record: job:http_requests_errors_5xx:rate1h
          expr: sum(rate(http_requests_total{job=~"available-schedules-(python|go)",status=~"5.."}[1h])) by (job)
          labels:
            metric_type: error_rate
        
        # Taxa de erro percentual (5min)
        - record: job:http_error_rate:ratio5m
          expr: |
            sum(rate(http_requests_total{job=~"available-schedules-(python|go)",status=~"5.."}[5m])) by (job)
            / 
            sum(rate(http_requests_total{job=~"available-schedules-(python|go)"}[5m])) by (job)
          labels:
            metric_type: error_ratio
        
        # Taxa de erro percentual (1h)
        - record: job:http_error_rate:ratio1h
          expr: |
            sum(rate(http_requests_total{job=~"available-schedules-(python|go)",status=~"5.."}[1h])) by (job)
            / 
            sum(rate(http_requests_total{job=~"available-schedules-(python|go)"}[1h])) by (job)
          labels:
            metric_type: error_ratio
        
        # Taxa de erro TOTAL (5min)
        - record: http_error_rate:ratio5m
          expr: |
            sum(rate(http_requests_total{job=~"available-schedules-(python|go)",status=~"5.."}[5m]))
            / 
            sum(rate(http_requests_total{job=~"available-schedules-(python|go)"}[5m]))
          labels:
            metric_type: error_ratio
            service: available-schedules
        
        # Taxa de erro TOTAL (1h)
        - record: http_error_rate:ratio1h
          expr: |
            sum(rate(http_requests_total{job=~"available-schedules-(python|go)",status=~"5.."}[1h]))
            / 
            sum(rate(http_requests_total{job=~"available-schedules-(python|go)"}[1h]))
          labels:
            metric_type: error_ratio
            service: available-schedules
        
        # Latência p50 por serviço (5min)
        - record: job:http_latency_p50:5m
          expr: |
            histogram_quantile(0.50, 
              sum by (job, le)(rate(http_request_duration_seconds_bucket{
                job=~"available-schedules-(python|go)"
              }[5m]))
            )
          labels:
            metric_type: latency
        
        # Latência p95 por serviço (5min)
        - record: job:http_latency_p95:5m
          expr: |
            histogram_quantile(0.95, 
              sum by (job, le)(rate(http_request_duration_seconds_bucket{
                job=~"available-schedules-(python|go)"
              }[5m]))
            )
          labels:
            metric_type: latency
        
        # Latência p95 por serviço (1h)
        - record: job:http_latency_p95:1h
          expr: |
            histogram_quantile(0.95, 
              sum by (job, le)(rate(http_request_duration_seconds_bucket{
                job=~"available-schedules-(python|go)"
              }[1h]))
            )
          labels:
            metric_type: latency
        
        # Latência p99 por serviço (5min)
        - record: job:http_latency_p99:5m
          expr: |
            histogram_quantile(0.99, 
              sum by (job, le)(rate(http_request_duration_seconds_bucket{
                job=~"available-schedules-(python|go)"
              }[5m]))
            )
          labels:
            metric_type: latency
        
        # Latência p95 TOTAL (5min)
        - record: http_latency_p95:5m
          expr: |
            histogram_quantile(0.95, 
              sum by (le)(rate(http_request_duration_seconds_bucket{
                job=~"available-schedules-(python|go)"
              }[5m]))
            )
          labels:
            metric_type: latency
            service: available-schedules
        
        # Latência p95 TOTAL (1h)
        - record: http_latency_p95:1h
          expr: |
            histogram_quantile(0.95, 
              sum by (le)(rate(http_request_duration_seconds_bucket{
                job=~"available-schedules-(python|go)"
              }[1h]))
            )
          labels:
            metric_type: latency
            service: available-schedules

    - name: app.availability
      rules:
        # ─────────────────────────────────────────────────────────────────
        # ALERTA: HighErrorRate
        # ANTES: Janela 3m, for 1m, severity única (page)
        # DEPOIS: Multi-window 5m+1h, for 2m, 2 níveis (warning/page)
        # JUSTIFICATIVA: Multi-window elimina 75-90% falso-positivos de
        #   spikes temporários. Thresholds graduados (0.5-3% warning, >3% 
        #   page) permitem roteamento adequado. Burn rate (1-6× vs 6×+) 
        #   contextualiza: 1× = 30 dias, 6× = 5 dias para esgotar budget.
        # ─────────────────────────────────────────────────────────────────
        - alert: HighErrorRateWarning
          expr: |
            # JANELA CURTA
            (http_error_rate:ratio5m > 0.005)
            and
            # JANELA LONGA
            (http_error_rate:ratio1h > 0.005)
            and
            # LIMITADOR
            (http_error_rate:ratio5m < 0.03)
          for: 2m
          labels:
            severity: warning
            service: available-schedules
            team: backend-team
            slo: "99.5"
          annotations:
            summary: "Taxa de erro entre 0.5-3%"
            description: |
              Taxa de erro atual: {{ $value }}
              SLO: 99.5% (error budget 0.5%)
              Janelas: 5min E 1h confirmam tendência sustentada
              Burn rate: Entre 1× e 6× (consome budget em 5-30 dias)
              
              Dashboard: http://dev.local/grafana/d/877494b4-6887-463c-9333-96c2b1fbf2b7
              Runbook: https://github.com/feegow/devops-trial-task/blob/main/runbooks/erro-5xx.md

        - alert: HighErrorRatePage
          expr: |
            # JANELA CURTA
            (http_error_rate:ratio5m > 0.03)
            and
            # JANELA LONGA
            (http_error_rate:ratio1h > 0.015)
          for: 2m
          labels:
            severity: page
            service: available-schedules
            team: backend-team
            slo: "99.5"
          annotations:
            summary: "URGENTE: Taxa de erro >3%"
            description: |
              Taxa de erro atual: {{ $value }}
              SLO: 99.5% (error budget 0.5%)
              Janelas: 5min E 1h confirmam degradação sustentada
              Burn rate: 6×+ (error budget esgota em 5 dias)
              
              Dashboard: http://dev.local/grafana/d/877494b4-6887-463c-9333-96c2b1fbf2b7
              Runbook: https://github.com/feegow/devops-trial-task/blob/main/runbooks/erro-5xx.md

        # ─────────────────────────────────────────────────────────────────
        # ALERTA: HighLatencyP95
        # ANTES: Janela 3m, for 1m, histogram_quantile complexo, 150ms
        # DEPOIS: Multi-window 5m+1h, for 2m, recording rules, 500/800ms
        # JUSTIFICATIVA: Recording rules economizam ~30% CPU Prometheus e
        #   padronizam dashboards. Thresholds UX-driven (Nielsen): 500ms =
        #   "instantâneo", 800ms = frustração. Multi-window filtra ~80%
        #   alertas de cold starts/deploys mantendo detecção em 2-4min.
        # ─────────────────────────────────────────────────────────────────
        - alert: HighLatencyP95Warning
          expr: |
            # JANELA CURTA
            (http_latency_p95:5m > 0.5)
            and
            # JANELA LONGA
            (http_latency_p95:1h > 0.5)
            and
            # LIMITADOR
            (http_latency_p95:5m < 0.8)
          for: 2m
          labels:
            severity: warning
            service: available-schedules
            team: backend-team
            slo: "99.5"
          annotations:
            summary: "Latência p95 entre 500-800ms"
            description: |
              Latência p95 atual: {{ $value }} segundos
              SLO latência: p95 < 500ms (99.5% das requisições)
              Janelas: 5min E 1h confirmam latência sustentada
              Burn rate: Entre 1× e 6×
            
              Dashboard: http://dev.local/grafana/d/877494b4-6887-463c-9333-96c2b1fbf2b7
              Runbook: https://github.com/feegow/devops-trial-task/blob/main/runbooks/erro-5xx.md

        - alert: HighLatencyP95Page
          expr: |
            # JANELA CURTA
            (http_latency_p95:5m > 0.8)
            and
            # JANELA LONGA
            (http_latency_p95:1h > 0.4)
          for: 2m
          labels:
            severity: page
            service: available-schedules
            team: backend-team
            slo: "99.5"                                 
          annotations:
            summary: "URGENTE: Latência p95 >800ms"
            description: |
              Latência p95 atual: {{ $value }} segundos
              SLO latência: p95 < 500ms (99.5% das requisições)
              Janelas: 5min E 1h confirmam degradação crítica
              Burn rate: 6×+ (latency budget esgota rapidamente)
              
              Dashboard: http://dev.local/grafana/d/877494b4-6887-463c-9333-96c2b1fbf2b7
              Runbook: https://github.com/feegow/devops-trial-task/blob/main/runbooks/erro-5xx.md